<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenAI Flask App</title>
</head>
<body>
    <h1>OpenAI Flask App</h1>
    <form method="post">
        <label for="question">Enter your question:</label>
        <input type="text" name="question" required>
        <br>
        <label for="api_key">Enter your OpenAI API key (not stored anywhere):</label>
        <input type="text" name="api_key" required>
        <br>
        <button type="submit">Submit</button>
    </form>

     <div style="margin-top: 20px; border-top: 1px solid #ccc; padding-top: 10px;">
        <h2>Challenges Faced (Something to read while you wait):</h2>
        <ul>
            <li>I tried using open-source LLMs, unfortunately my system did not have enough RAM to accommodate large models. Hence, I decided to use GPT API.</li>
            <li>GPT Api has a token limit of 4096 and therefore large information could not be sent to the model so often data was limited. Hence, we don't get the best answers</li>
            <li>Scraping each search result sequentially took time. We can parallelize content retrieval</li>
        </ul>
    </div>

    {% if result %}
        <h2>Response:</h2>
        <p>{{ result }}</p>
    {% endif %}
</body>
</html>
